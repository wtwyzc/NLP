{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T13:41:09.920660Z",
     "start_time": "2023-04-24T13:41:06.501267Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m  \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m  \u001b[38;5;66;03m#正则表达式\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwordsci\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "import re  #正则表达式\n",
    "import nltk \n",
    "from nltk.corpus import stopwordsci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 删除html页面标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T13:41:09.936472Z",
     "start_time": "2023-04-24T13:41:09.923646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Big data refers to the large, diverse sets of information that grow at ever-increasing rates. \n",
      "        It encompasses the volume of information, the velocity or speed at which it is created and collected, \n",
      "        and the variety or scope of the data points being covered. \n",
      "        Big data often comes from multiple sources and arrives in multiple formats.\n"
     ]
    }
   ],
   "source": [
    "#删除html页面标记\n",
    "text = '''<p><font> Big data refers to the large, diverse sets of information that grow at ever-increasing rates. \n",
    "        It encompasses the volume of information, the velocity or speed at which it is created and collected, \n",
    "        and the variety or scope of the data points being covered. \n",
    "        Big data often comes from multiple sources and arrives in multiple formats.</p><br//>'''\n",
    "\n",
    "#compile函数生成正则表达式pattern，匹配html标签<>. re.S 匹配包括换行在内的所有字符\n",
    "pattern = re.compile(r'<[^>]+>',re.S) \n",
    "result = pattern.sub('', text) #用空字符替换掉匹配的html标记\n",
    "\n",
    "result.lower()  #英文转换小写\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去除标点及停用词 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T13:41:09.999306Z",
     "start_time": "2023-04-24T13:41:09.938437Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m result\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,result)  \u001b[38;5;66;03m#正则表达式，去除标点符号\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m tokenize \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mword_tokenize(result) \u001b[38;5;66;03m#英文分词\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tokenize_new \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokenize \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)] \u001b[38;5;66;03m#去除停用词\u001b[39;00m\n\u001b[1;32m      6\u001b[0m tokenize_new\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "result=re.sub(r'[^\\w\\s]','',result)  #正则表达式，去除标点符号\n",
    "\n",
    "tokenize = nltk.word_tokenize(result) #英文分词\n",
    "\n",
    "tokenize_new = [w for w in tokenize if not w in stopwords.words('english')] #去除停用词\n",
    "tokenize_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词干提取 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T13:41:10.015239Z",
     "start_time": "2023-04-24T13:41:10.002269Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#词干提取\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;241m.\u001b[39mSnowballStemmer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n\u001b[1;32m      3\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m [s\u001b[38;5;241m.\u001b[39mstem(ws) \u001b[38;5;28;01mfor\u001b[39;00m ws \u001b[38;5;129;01min\u001b[39;00m tokenize_new]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(stemmer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "#词干提取\n",
    "s = nltk.stem.SnowballStemmer('english')  \n",
    "stemmer = [s.stem(ws) for ws in tokenize_new]\n",
    "print(stemmer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T13:41:10.190765Z",
     "start_time": "2023-04-24T13:41:10.020222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Big', 'NNP'), ('data', 'NN'), ('refers', 'NNS'), ('large', 'JJ'), ('diverse', 'JJ'), ('sets', 'NNS'), ('information', 'NN'), ('grow', 'NN'), ('everincreasing', 'VBG'), ('rates', 'NNS'), ('It', 'PRP'), ('encompasses', 'VBZ'), ('volume', 'NN'), ('information', 'NN'), ('velocity', 'NN'), ('speed', 'NN'), ('created', 'VBD'), ('collected', 'JJ'), ('variety', 'NN'), ('scope', 'NN'), ('data', 'NNS'), ('points', 'NNS'), ('covered', 'VBD'), ('Big', 'NNP'), ('data', 'NNS'), ('often', 'RB'), ('comes', 'VBZ'), ('multiple', 'JJ'), ('sources', 'NNS'), ('arrives', 'VBZ'), ('multiple', 'JJ'), ('formats', 'NNS')]\n",
      "large diverse everincreasing encompasses created collected covered often comes multiple arrives multiple\n"
     ]
    }
   ],
   "source": [
    "#词性标注\n",
    "pos_tags=nltk.pos_tag(tokenize_new)#词性标注\n",
    "print(pos_tags)\n",
    "tags = set(['MD', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'RP', 'RB', 'RBR', 'RBS', 'JJ', 'JJR', 'JJS'])#根据词性选择\n",
    "ret = []\n",
    "for word,pos in pos_tags:\n",
    "        if (pos in tags):\n",
    "            ret.append(word)\n",
    "newdata= ' '.join(ret)\n",
    "print(newdata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T13:41:11.928117Z",
     "start_time": "2023-04-24T13:41:10.193755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Big', 'data', 'refers', 'large', 'diverse', 'set', 'information', 'grow', 'everincreasing', 'rate', 'encompass', 'volume', 'information', 'velocity', 'speed', 'create', 'collected', 'variety', 'scope', 'data', 'point', 'cover', 'Big', 'data', 'often', 'come', 'multiple', 'source', 'arrive', 'multiple', 'format']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "words_lemma=[]\n",
    "for pos in pos_tags:  #pos_tags 上一步词性标注结果\n",
    "    if get_wordnet_pos(pos[1])!=None : #根据上面函数，J,V,N,R开头的保留，其他返回空值，如果不为空，表示如果为JVNR\n",
    "        wordnet_pos =get_wordnet_pos(pos[1]) #pos[1]为标注的词性\n",
    "        words_lemma.append(wnl.lemmatize(pos[0], pos=wordnet_pos)) # 词形还原     \n",
    "print(words_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词频统计 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T13:41:11.944089Z",
     "start_time": "2023-04-24T13:41:11.932107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big:2\n",
      "data:3\n",
      "refers:1\n",
      "large:1\n",
      "diverse:1\n",
      "set:1\n",
      "information:2\n",
      "grow:1\n",
      "everincreasing:1\n",
      "rate:1\n",
      "encompass:1\n",
      "volume:1\n",
      "velocity:1\n",
      "speed:1\n",
      "create:1\n",
      "collected:1\n",
      "variety:1\n",
      "scope:1\n",
      "point:1\n",
      "cover:1\n",
      "often:1\n",
      "come:1\n",
      "multiple:2\n",
      "source:1\n",
      "arrive:1\n",
      "format:1\n"
     ]
    }
   ],
   "source": [
    "#词频统计\n",
    "from nltk import FreqDist    \n",
    "freq = FreqDist(words_lemma)    #words_lemma:上一步词形还原结果\n",
    "for key,val in freq.items():\n",
    "    print (str(key) + ':' + str(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词云 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T13:41:12.560089Z",
     "start_time": "2023-04-24T13:41:11.948082Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words_lemma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#词云\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m----> 3\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mwords_lemma\u001b[49m)\n\u001b[1;32m      4\u001b[0m wc \u001b[38;5;241m=\u001b[39m WordCloud(background_color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m,width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m)\n\u001b[1;32m      5\u001b[0m wwc \u001b[38;5;241m=\u001b[39m wc\u001b[38;5;241m.\u001b[39mgenerate(words)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words_lemma' is not defined"
     ]
    }
   ],
   "source": [
    "#词云\n",
    "from wordcloud import WordCloud\n",
    "words = ' '.join(words_lemma)\n",
    "wc = WordCloud(background_color = 'white',width=800, height=600)\n",
    "wwc = wc.generate(words)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wwc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T13:41:13.385061Z",
     "start_time": "2023-04-24T13:41:12.563083Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Yjh\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.763 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果 南京市/ 长江大桥\n"
     ]
    }
   ],
   "source": [
    "#中文分词\n",
    "import jieba \n",
    "sent = '南京市长江大桥'  \n",
    "seg_list = jieba.cut(sent)    #jieba分词默认模式 \n",
    "print('分词结果', '/ '.join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T13:41:13.891187Z",
     "start_time": "2023-04-24T13:41:13.390020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文/nz 分词/n 是/v 文本处理/n 不可或缺/l 的/uj 一步/m ，/x 地球/n 人/n 加油/v ！/x\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as psg \n",
    "\n",
    "sent = '中文分词是文本处理不可或缺的一步，地球人加油！'  \n",
    "seg_list = psg.cut(sent)  # Jieba词性标注\n",
    "print(' '.join(['{0}/{1}'.format(w, t) for w, t in seg_list]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本表示 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词袋子 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T13:41:13.906185Z",
     "start_time": "2023-04-24T13:41:13.895178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'and this' 'document' 'document is' 'first' 'first document' 'is'\n",
      " 'is the' 'is this' 'one' 'second' 'second document' 'the' 'the first'\n",
      " 'the second' 'the third' 'third' 'third one' 'this' 'this document'\n",
      " 'this is' 'this the']\n",
      "[[0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0]\n",
      " [0 0 2 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0]\n",
      " [1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0]\n",
      " [0 0 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?']\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2)) #创建词袋子  1元 和2元，默认1元\n",
    "dt = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(dt.toarray())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T13:41:13.938064Z",
     "start_time": "2023-04-24T13:41:13.911136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'and this' 'document' 'document is' 'first' 'first document' 'is'\n",
      " 'is the' 'is this' 'one' 'second' 'second document' 'the' 'the first'\n",
      " 'the second' 'the third' 'third' 'third one' 'this' 'this document'\n",
      " 'this is' 'this the']\n",
      "[[0.         0.         0.3145322  0.         0.38850984 0.38850984\n",
      "  0.25715068 0.3145322  0.         0.         0.         0.\n",
      "  0.25715068 0.38850984 0.         0.         0.         0.\n",
      "  0.25715068 0.         0.38850984 0.        ]\n",
      " [0.         0.         0.45551258 0.35682424 0.         0.\n",
      "  0.18620569 0.22775629 0.         0.         0.35682424 0.35682424\n",
      "  0.18620569 0.         0.35682424 0.         0.         0.\n",
      "  0.18620569 0.35682424 0.         0.        ]\n",
      " [0.35700721 0.35700721 0.         0.         0.         0.\n",
      "  0.18630117 0.22787308 0.         0.35700721 0.         0.\n",
      "  0.18630117 0.         0.         0.35700721 0.35700721 0.35700721\n",
      "  0.18630117 0.         0.28146859 0.        ]\n",
      " [0.         0.         0.28293955 0.         0.34948664 0.34948664\n",
      "  0.23132162 0.         0.44327948 0.         0.         0.\n",
      "  0.23132162 0.34948664 0.         0.         0.         0.\n",
      "  0.23132162 0.         0.         0.44327948]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))#参数默认1元\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T13:41:14.016429Z",
     "start_time": "2023-04-24T13:41:13.944046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.89256849 -0.09143315 -0.18907985]\n",
      " [ 0.59110831 -0.04894607  0.8032774 ]\n",
      " [ 0.47733484  0.86478807 -0.11173105]\n",
      " [ 0.80235771 -0.37670319 -0.31497686]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd=TruncatedSVD(3)\n",
    "X1=svd.fit_transform(X)\n",
    "print(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
